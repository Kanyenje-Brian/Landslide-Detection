{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12572787,"sourceType":"datasetVersion","datasetId":7939827}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Landslide Detection Challenge - Starter Notebook\n\nWelcome to the Landslide Detection Challenge! This notebook will guide you through:\n1. Loading and exploring the multi-band dataset provided in `.npy` format.\n2. Visualizing the multi-band satellite data and understanding label distribution.\n3. Building and evaluating a baseline model to classify landslide and non-landslide images.\n\nLet’s get started with loading and understanding the data!\n","metadata":{"id":"nODaxSas0JbR"}},{"cell_type":"markdown","source":"## Block 1: Import Libraries","metadata":{"id":"ejfTrwF10JbV"}},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport plotly.express as px\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, f1_score, make_scorer\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import load_model\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.ensemble import RandomForestClassifier\nimport tensorflow as tf\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom optuna.samplers import RandomSampler\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom xgboost import XGBClassifier\nimport optuna","metadata":{"id":"K1NuFY_x0JbW","trusted":true,"execution":{"iopub.status.busy":"2025-07-29T16:52:11.702908Z","iopub.execute_input":"2025-07-29T16:52:11.704018Z","iopub.status.idle":"2025-07-29T16:52:15.005129Z","shell.execute_reply.started":"2025-07-29T16:52:11.703982Z","shell.execute_reply":"2025-07-29T16:52:15.004007Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### Explanation\nWe import the required libraries:\n- **os**: for file and directory handling.\n- **numpy**: for numerical operations, particularly for loading `.npy` files.\n- **pandas**: for data handling with CSV files.\n- **matplotlib.pyplot**: for visualizing data, such as label distributions.\n- **sklearn.model_selection.train_test_split**: for splitting data into training and validation sets.\n- **tensorflow.keras**: for building and training a neural network model.\n","metadata":{"id":"sVQRf-ws0JbX"}},{"cell_type":"markdown","source":"## Block 2: Define Paths and Load CSV Files","metadata":{"id":"wAPsl4Si0JbY"}},{"cell_type":"code","source":"# Define paths for the dataset (remember to unzip the dataset first!)\ntrain_csv_path = '/kaggle/input/landslide-detection/Train.csv'  # Path to the training labels CSV file\ntest_csv_path = '/kaggle/input/landslide-detection/Test.csv'    # Path to the test image IDs CSV file\ntrain_data_path = '/kaggle/input/landslide-detection/train_data/train_data'  # Folder where .npy train files are located\ntest_data_path = '/kaggle/input/landslide-detection/test_data/test_data'    # Folder where .npy test files are located\n\n# Load Train.csv and inspect the data\ntrain_df = pd.read_csv(train_csv_path)\ntest_df = pd.read_csv(test_csv_path)\nprint(\"Train.csv:\")\nprint(train_df.head())","metadata":{"id":"xDyACVkf0JbY","outputId":"b803a45c-fecd-4ac3-bd8d-be6e802ee661","trusted":true,"execution":{"iopub.status.busy":"2025-07-29T16:43:59.854873Z","iopub.execute_input":"2025-07-29T16:43:59.855146Z","iopub.status.idle":"2025-07-29T16:43:59.916688Z","shell.execute_reply.started":"2025-07-29T16:43:59.855123Z","shell.execute_reply":"2025-07-29T16:43:59.915727Z"}},"outputs":[{"name":"stdout","text":"Train.csv:\n          ID  label\n0  ID_HUD1ST      1\n1  ID_KGE2HY      1\n2  ID_VHV9BL      1\n3  ID_ZT0VEJ      0\n4  ID_5NFXVY      0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"train_df.label.value_counts()","metadata":{"id":"WRN5mqzw5wf8","outputId":"b6dd18be-3fee-45a2-9307-b223d809b9fc","trusted":true,"execution":{"iopub.status.busy":"2025-07-29T16:43:59.917807Z","iopub.execute_input":"2025-07-29T16:43:59.918189Z","iopub.status.idle":"2025-07-29T16:43:59.927108Z","shell.execute_reply.started":"2025-07-29T16:43:59.918153Z","shell.execute_reply":"2025-07-29T16:43:59.926237Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"label\n0    5892\n1    1255\nName: count, dtype: int64"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"### Explanation\n- **Define Paths**: Specify paths to `Train.csv`, `Test.csv`, and folders containing `.npy` files for training and testing images.\n- **Load Train.csv**: We read the `Train.csv` file, which contains `ID` and `label` columns. The `label` is binary, indicating whether the image contains a landslide (1) or not (0).\n","metadata":{"id":"lPDktPhp0JbZ"}},{"cell_type":"markdown","source":"## Block 4: Load, Normalize, and Display Sample Multi-band Images","metadata":{"id":"jida8k-a0Jbb"}},{"cell_type":"code","source":"# Function to load and normalize .npy images\ndef load_and_normalize_npy_image(image_id, folder_path):\n    \"\"\"Loads a .npy file, normalizes each band, and returns the normalized image.\"\"\"\n    image_path = os.path.join(folder_path, f\"{image_id}.npy\")\n    img = np.load(image_path, mmap_mode='r')\n\n    img = img.astype('float32')  # ensure float for normalization\n\n    # Normalize per band (channels last)\n    min_vals = img.min(axis=(0, 1), keepdims=True)\n    max_vals = img.max(axis=(0, 1), keepdims=True)\n    img_normalized = (img - min_vals) / (max_vals - min_vals + 1e-8)\n\n    return img_normalized\n\n# Band descriptions\nband_descriptions = [\n    \"Red\", \"Green\", \"Blue\", \"Near Infrared\",\n    \"Descending VV (Vertical-Vertical)\", \"Descending VH (Vertical-Horizontal)\",\n    \"Descending Diff VV\", \"Descending Diff VH\",\n    \"Ascending VV (Vertical-Vertical)\", \"Ascending VH (Vertical-Horizontal)\",\n    \"Ascending Diff VV\", \"Ascending Diff VH\"\n]\n\nX = np.array([load_and_normalize_npy_image(image_id, train_data_path) for image_id in train_df['ID']])\ny = train_df['label'].values\nX_test = np.array([load_and_normalize_npy_image(image_id, test_data_path) for image_id in test_df['ID']])","metadata":{"id":"9SgOyTHP0Jbb","outputId":"a165535e-150e-4e3c-e815-e434a3f158b4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Explanation\nThis block provides a complete view of the 12 individual bands with the corrected descriptions for ascending and descending radar bands.\n\n1. **Band Descriptions**:\n   - **Bands 1-4**: Visible and Near Infrared bands (Red, Green, Blue, NIR).\n   - **Bands 5-8**: Descending radar bands:\n     - **Band 5**: Descending VV (Vertical-Vertical polarization).\n     - **Band 6**: Descending VH (Vertical-Horizontal polarization).\n     - **Band 7**: Descending Diff VV.\n     - **Band 8**: Descending Diff VH.\n   - **Bands 9-12**: Ascending radar bands:\n     - **Band 9**: Ascending VV (Vertical-Vertical polarization).\n     - **Band 10**: Ascending VH (Vertical-Horizontal polarization).\n     - **Band 11**: Ascending Diff VV.\n     - **Band 12**: Ascending Diff VH.\n\n2. **Plotting Layout**:\n   - A 3x4 grid layout displays each band as a grayscale image.\n   - Each subplot includes the band number and description for easy reference.\n   - `plt.subplots_adjust` adds spacing between the plots to improve readability.\n","metadata":{"id":"zIsE_pDF0Jbc"}},{"cell_type":"markdown","source":"## Feature Extraction","metadata":{}},{"cell_type":"code","source":"# Path to the folder containing .npy images\n\ndef extract_features(img):\n    # img: (batch, H, W, 12)\n\n    # Bands 1-4: Optical\n    red, green, blue, nir = [img[:, :, :, i] for i in range(4)]\n    ndvi = (nir - red) / (nir + red + 1e-5)\n    ndwi = (green - nir) / (green + nir + 1e-5)\n\n    # Radar Descending\n    vv_desc, vh_desc = img[:, :, :, 4], img[:, :, :, 5]\n    vv_vh_ratio_desc = (vv_desc + 1e-5) / (vh_desc + 1e-5)\n\n    # Radar Ascending\n    vv_asc, vh_asc = img[:, :, :, 8], img[:, :, :, 9]\n    vv_vh_ratio_asc = (vv_asc + 1e-5) / (vh_asc + 1e-5)\n\n    # Expand NDVI etc. to (batch, H, W, 1) before concatenation\n    ndvi = ndvi[..., np.newaxis]\n    ndwi = ndwi[..., np.newaxis]\n    vv_vh_ratio_desc = vv_vh_ratio_desc[..., np.newaxis]\n    vv_vh_ratio_asc = vv_vh_ratio_asc[..., np.newaxis]\n\n    # Concatenate along channel axis\n    features = np.concatenate([img, ndvi, ndwi, vv_vh_ratio_desc, vv_vh_ratio_asc], axis=-1)\n    return features\n\n\nX_features = extract_features(X)\nX_features_test = extract_features(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Block 5: Prepare Data for Model Training","metadata":{"id":"SP3JYQyC0Jbc"}},{"cell_type":"code","source":"# Normalize\nX_train, X_val, y_train, y_val = train_test_split(\n    X_features, y_labels, \n    test_size=0.2, \n    stratify=y_labels, \n    random_state=SEED\n)\nX_train = X_train.astype('float32') / 255.0\nX_val = X_val.astype('float32') / 255.0\n\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\ntrain_ds = train_ds.shuffle(1024).batch(BATCH_SIZE)\n\n# Add custom augmentations\ndef augment(image, label):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    image = tf.image.rot90(image, tf.random.uniform([], 0, 4, dtype=tf.int32))\n    return image, label\n\ntrain_ds = train_ds.map(augment)\n\nval_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Explanation\n\n1. **Load Data**:\n   - We define `load_npy_image` to load `.npy` files as raw images.\n   - `X` is created by loading each image using `load_npy_image` based on the image IDs in `train_df`.\n   - `y` contains the labels from `train_df`.\n\n2. **Stratified Data Split**:\n   - We split the data into `X_train`, `X_val`, `y_train`, and `y_val` while preserving class distribution using `stratify=y`.\n\n3. **ImageDataGenerator for Training**:\n   - `train_datagen` is configured with data augmentation options to increase the diversity of the training data.\n\n4. **ImageDataGenerator for Validation**:\n   - `val_datagen` loads the validation data without augmentation.\n\n5. **Generators**:\n   - `train_ds` and `val_ds` are created using `.flow()`, which yields data in batches for efficient training and validation.","metadata":{"id":"JvYIWDAb0Jbc"}},{"cell_type":"markdown","source":"## Block 6: Define and Compile a CNN Model with Focal Loss","metadata":{"id":"o6xK9j0k0Jbc"}},{"cell_type":"code","source":"# Precision\ndef precision_m(y_true, y_pred):\n    y_true = K.cast(y_true, 'float32')\n    y_pred = K.round(K.clip(y_pred, 0, 1))   # ✅ Threshold predictions\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\n# Recall\ndef recall_m(y_true, y_pred):\n    y_true = K.cast(y_true, 'float32')\n    y_pred = K.round(K.clip(y_pred, 0, 1))   # ✅ Threshold predictions\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\n# F1 Score\ndef f1_m(y_true, y_pred):\n    p = precision_m(y_true, y_pred)\n    r = recall_m(y_true, y_pred)\n    return 2 * ((p * r) / (p + r + K.epsilon()))\n\n# Define the Focal Loss function\ndef focal_loss(gamma=2.0, alpha=0.25):\n    \"\"\"\n    Focal Loss for binary classification.\n\n    Parameters:\n        gamma (float): Focusing parameter; typically set to 2.0.\n        alpha (float): Balancing factor; typically set to 0.25.\n\n    Returns:\n        Binary Focal Loss function.\n    \"\"\"\n    def focal_loss_fixed(y_true, y_pred):\n        # Clip predictions to prevent log(0)\n        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n\n        # Calculate p_t\n        p_t = tf.where(K.equal(y_true, 1), y_pred, 1 - y_pred)\n\n        # Calculate focal loss\n        fl = -alpha * K.pow(1 - p_t, gamma) * K.log(p_t)\n        return K.mean(fl)\n\n    return focal_loss_fixed\n\nmodel = Sequential([\n    # First convolutional block\n    Input(shape=X_train.shape[1:]),\n    Conv2D(32, (3, 3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D((2, 2)),\n    Dropout(0.25),\n\n    # Second convolutional block\n    Conv2D(64, (3, 3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D((2, 2)),\n    Dropout(0.25),\n\n    # Third convolutional block\n    Conv2D(128, (3, 3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D((2, 2)),\n    Dropout(0.25),\n\n    # Fourth convolutional block for deeper feature extraction\n    Conv2D(256, (3, 3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D((2, 2)),\n    Dropout(0.5),\n\n    # Flatten and add dense layers\n    Flatten(),\n    Dense(128, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.5),  # Dropout for regularization\n    Dense(64, activation='relu'),\n    BatchNormalization(),\n    Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n])\n\n\n# Compile the model with Focal Loss and additional metrics\nmodel.compile(\n    optimizer='adam',\n    loss=focal_loss(gamma=2.0, alpha=0.5),\n    metrics=['accuracy']  # Additional metrics\n)\n\n# Display the model summary\nmodel.summary()","metadata":{"id":"uG2-yhmH0Jbc","outputId":"da9f574e-075f-4d44-fd8b-8b303249d880","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Explanation\n\nThis code defines a Convolutional Neural Network (CNN) with custom metrics (Precision, Recall, and F1 Score) and **Focal Loss** for training, making it suitable for imbalanced datasets.\n\n#### Key Components\n\n1. **Custom Metrics**:\n   - `precision_m`: Calculates the proportion of true positive predictions out of all positive predictions, which helps evaluate the model’s accuracy in predicting positive (landslide) samples.\n   - `recall_m`: Measures the proportion of true positives out of all actual positives, reflecting the model’s ability to detect all positive cases.\n   - `f1_m`: Combines Precision and Recall into a single score using the harmonic mean, making it useful for evaluating the model on imbalanced datasets.\n\n2. **Focal Loss Function**:\n   - Focal Loss is designed to focus on hard-to-classify examples, making it particularly beneficial for imbalanced datasets.\n   - Parameters:\n     - `gamma=2.0`: Adjusts the focusing mechanism. Higher values place more focus on misclassified examples.\n     - `alpha=0.25`: Balances the contribution of positive and negative samples, ensuring the loss calculation doesn’t get dominated by the majority class.\n   - The function `focal_loss_fixed` calculates Focal Loss by:\n     - Clipping predictions to avoid `log(0)`.\n     - Calculating the probability for each prediction (`p_t`), where correct predictions contribute less to the loss.\n     - Applying the focal scaling factor `(1 - p_t)^{\\gamma}` to emphasize harder examples in the loss computation.\n\n3. **CNN Model Architecture**:\n   - The CNN is designed with four convolutional blocks, each containing:\n     - **Conv2D layers**: Extract features with increasing complexity as the model goes deeper.\n     - **BatchNormalization layers**: Normalize activations, speeding up convergence and improving stability.\n     - **MaxPooling2D layers**: Down-sample feature maps, reducing spatial dimensions and capturing abstract patterns.\n     - **Dropout layers**: Applied with increasing rates, reducing overfitting by randomly deactivating nodes during training.\n   - Following the convolutional blocks:\n     - **Flatten**: Converts 2D feature maps to a 1D vector.\n     - **Dense layers**: Two fully connected layers with ReLU activation capture higher-level features, with Dropout for regularization.\n     - **Sigmoid Output Layer**: Used for binary classification, outputs the probability of each class (No Landslide or Landslide).\n\n4. **Model Compilation**:\n   - `optimizer='adam'`: An adaptive optimizer that adjusts the learning rate automatically during training.\n   - `loss=focal_loss(gamma=2.0, alpha=0.25)`: Focal Loss to handle class imbalance.\n   - `metrics=['accuracy', precision_m, recall_m, f1_m]`: Additional metrics for a comprehensive evaluation of the model's performance on imbalanced data.\n\n5. **Model Summary**:\n   - `model.summary()` displays the model’s architecture, showing layer types, output shapes, and parameter counts. This summary helps verify that the model structure matches expectations before training begins.","metadata":{"id":"Rg__4FUJ0Jbd"}},{"cell_type":"markdown","source":"## Block 7: Train the Model","metadata":{"id":"7ukG0Zjk0Jbd"}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n\n# Create a checkpoint callback that saves the best model based on validation loss\ncallback = [ModelCheckpoint(\n                \"kaggle/working/best_model.h5\",            \n                monitor='val_loss',   \n                verbose=1,                  \n                save_best_only=True,       \n                mode='min'),\n            \n            EarlyStopping(\n                monitor='val_loss', patience=5)\n           ]\n\n#class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n#class_weights_dict = dict(enumerate(class_weights))\n# Train the model using the generators with the checkpoint callback\nhistory = model.fit(\n    train_ds,       # Train generator\n    epochs=50,\n    validation_data=val_ds,  # Validation generator\n    callbacks=[callback],  # Include the checkpoint callback in training\n)\n","metadata":{"id":"qL32Y2Gi0Jbd","outputId":"e609cea7-aedd-4745-cc4b-f77b63690c41","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMG_SIZE = 224\nBATCH_SIZE = 32\nAUTOTUNE = tf.data.AUTOTUNE\nEPOCHS = 20\nN_SPLITS = 5\n\n# ✅ Encode labels\nclass_names = sorted(train_df['label'].unique())\nlabel_to_index = {name: i for i, name in enumerate(class_names)}\ntrain_df['label_idx'] = train_df['label'].map(label_to_index)\n\npaths = train_df['image_path'].values\nlabels = train_df['label_idx'].values\n\n# ✅ Load function\ndef load_image(path, label, training=True):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n\n    if training:\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_flip_up_down(img)\n        img = tf.image.random_brightness(img, 0.2)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n\n    return img, label\n\n\ndef build_dataset(x_paths, y_labels, training=True):\n    ds = tf.data.Dataset.from_tensor_slices((x_paths, y_labels))\n    ds = ds.map(load_image, num_parallel_calls=AUTOTUNE)\n    if training:\n        ds = ds.shuffle(1024, seed=42)\n    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n    return ds\n\n# ✅ Model\ndef build_efficientnetb3():\n    base_model = tf.keras.applications.EfficientNetB3(\n        include_top=False,\n        weights=\"imagenet\",\n        input_shape=(IMG_SIZE, IMG_SIZE, 3)\n    )\n    base_model.trainable = False\n\n    inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n    x = tf.keras.applications.efficientnet.preprocess_input(inputs)\n    x = base_model(x, training=False)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dropout(0.3)(x)\n    outputs = tf.keras.layers.Dense(len(class_names), activation=\"softmax\")(x)\n\n    model = tf.keras.Model(inputs, outputs)\n    base_model.trainable = True\n    for layer in base_model.layers[:100]:\n        layer.trainable = False \n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(1e-4),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n    return model\n\n# ✅ StratifiedKFold loop\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(paths, labels)):\n    print(f\"\\n🔹 Fold {fold+1}/{N_SPLITS}\")\n    \n    train_paths, val_paths = paths[train_idx], paths[val_idx]\n    train_labels, val_labels = labels[train_idx], labels[val_idx]\n\n    train_ds = build_dataset(train_paths, train_labels, training=True)\n    val_ds = build_dataset(val_paths, val_labels, training=False)\n\n    # ✅ Compute class weights for this fold\n    cw_values = compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(train_labels),\n        y=train_labels\n    )\n    class_weights = dict(enumerate(cw_values))\n    print(\"Class Weights:\", class_weights)\n\n    # ✅ Build and train model\n    model = build_efficientnetb3()\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True, monitor='val_loss')\n    ]\n\n    model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=EPOCHS,\n        class_weight=class_weights,\n        callbacks=callbacks,\n        verbose=1\n    )\n\n    # Get predictions\n    y_true = []\n    y_pred = []\n    \n    for images, labels in val_ds:\n        preds = model.predict(images)\n        y_true.extend(labels.numpy())\n        y_pred.extend(np.argmax(preds, axis=1))\n    \n    # Compute F1\n    f1 = f1_score(y_true, y_pred) \n    print(\"Validation F1 Score:\", f1)\n    del model\n    gc.collect()\n    tf.keras.backend.clear_session()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Explanation\n\n1. **Model Training**:\n   - `model.fit` is updated to use the `train_generator` and `val_generator`.\n   - `steps_per_epoch` and `validation_steps` control how many batches are processed per epoch for training and validation.\n2. **Efficiency**:\n   - Using a generator allows the model to load data in batches, reducing memory usage and making training feasible for large datasets.","metadata":{"id":"rDA2rhFy0Jbe"}},{"cell_type":"markdown","source":"## Block 8: Plot Training and Validation Accuracy","metadata":{"id":"HWs31OAA0Jbe"}},{"cell_type":"code","source":"def evaluate_model(model, X_test):\n\n    # Predict probabilities\n    y_pred = model.predict(X_test)\n    y_pred = (y_pred > 0.5).astype(int)\n    # Compute F1 score\n    f1 = f1_score(y_test, y_pred)\n    print(f\"F1 Score: {f1:.4f}\\n\")\n\n    # Confusion Matrix\n    cm = confusion_matrix(y_test, y_pred)\n    plt.figure(figsize=(6,5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n\n    # Detailed report\n    print(classification_report(y_test, y_pred))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = model.predict(val_gen)\ny_pred = (predictions > 0.5).astype(int)\ny_test = val_df['label']\ncm = confusion_matrix(val_df['label'], y_pred)\ndisp = ConfusionMatrixDisplay(cm)\ndisp.plot()\nprint(classification_report(y_test, y_pred))\nprint(f1_score(y_test, y_pred, average='binary'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def threshold_tuning(model):\n    scores = []\n    thresholds = np.linspace(0.1, 1, 10)\n    for thresh in thresholds:\n        y_pred = model.predict(X_test)\n        y_tuned = (y_pred > thresh).astype(int)\n        scores.append(f1_score(y_test, y_tuned))\n\n    fig = px.line(x=thresholds, y=scores, title='Threshold Tuning', template='plotly_white')\n    fig.update_traces(line=dict(color='red'))\n    fig.update_layout(xaxis_title='Thresholds',\n                      yaxis_title='F1 Scores',\n                      height=500,\n                      width=800)\n    \n    fig.show()\n\nthreshold_tuning(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training and validation accuracy and loss\nplt.figure(figsize=(12, 5))\n\n# Plot accuracy\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Training and Validation Accuracy')\n\n# Plot loss\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Training and Validation Loss')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"TVNNaQvJ0Jbe","outputId":"a035e2f5-b66b-4a84-ab7c-8b16071c8628","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Explanation\nThis plot shows the **training and validation accuracy** as well as the **training and validation loss** over the epochs, allowing us to visually inspect the model’s learning behavior:\n\n- **Steady Improvements in Both Accuracy and Loss**: Consistent increases in accuracy and decreases in loss for both training and validation sets indicate effective learning and good generalization.\n\n- **Divergence Between Training and Validation Metrics**:\n  - If **training accuracy is high** but **validation accuracy is much lower** (with validation loss increasing), this suggests **overfitting**. The model may perform well on training data but fails to generalize to new data.\n  - If both **training and validation accuracy remain low** and losses are high, this indicates **underfitting**, meaning the model may not be complex enough to capture patterns in the data.\n\nThis combined plot of accuracy and loss offers a comprehensive view of model performance, helping us assess both how well the model fits the training data and how well it generalizes to new, unseen data.\n","metadata":{"id":"44qd1L2o0Jbe"}},{"cell_type":"markdown","source":"# Ensemble Methods","metadata":{}},{"cell_type":"code","source":"def load_image(image_id, folder_path):\n    image_path = os.path.join(folder_path, f\"{image_id}.npy\")\n    img = np.load(image_path, mmap_mode='r')\n    img = img.astype('float32')\n    return img\n\nX = np.array([load_image(image_id, train_data_path) for image_id in train_df['ID']])\nX_test_arrays = np.array([load_image(image_id, test_data_path) for image_id in test_df['ID']])\ny = train_df['label'].values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T16:44:11.986871Z","iopub.execute_input":"2025-07-29T16:44:11.987326Z","iopub.status.idle":"2025-07-29T16:45:28.662327Z","shell.execute_reply.started":"2025-07-29T16:44:11.987292Z","shell.execute_reply":"2025-07-29T16:45:28.661351Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def extract_features(img):\n    # img: (batch, H, W, 12)\n\n    # Bands 1-4: Optical\n    red, green, blue, nir = [img[:, :, :, i] for i in range(4)]\n    ndvi = (nir - red) / (nir + red + 1e-5)\n    ndwi = (green - nir) / (green + nir + 1e-5)\n\n    # Radar Descending\n    vv_desc, vh_desc = img[:, :, :, 4], img[:, :, :, 5]\n    vv_vh_ratio_desc = (vv_desc + 1e-5) / (vh_desc + 1e-5)\n\n    # Radar Ascending\n    vv_asc, vh_asc = img[:, :, :, 8], img[:, :, :, 9]\n    vv_vh_ratio_asc = (vv_asc + 1e-5) / (vh_asc + 1e-5)\n\n    # Expand NDVI etc. to (batch, H, W, 1) before concatenation\n    ndvi = ndvi[..., np.newaxis]\n    ndwi = ndwi[..., np.newaxis]\n    vv_vh_ratio_desc = vv_vh_ratio_desc[..., np.newaxis]\n    vv_vh_ratio_asc = vv_vh_ratio_asc[..., np.newaxis]\n\n    # Concatenate along channel axis\n    features = np.concatenate([img, ndvi, ndwi], axis=-1)\n    return features\n\nX_features = extract_features(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T17:38:01.497933Z","iopub.execute_input":"2025-07-29T17:38:01.498342Z","iopub.status.idle":"2025-07-29T17:38:03.810761Z","shell.execute_reply.started":"2025-07-29T17:38:01.498310Z","shell.execute_reply":"2025-07-29T17:38:03.809787Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"def means(df, X):\n    # Bands 4 to 12 → index 3 to 11\n    df1 = df.copy()\n    for i, band in enumerate(range(12)):\n        df1[f'band{band+1}_mean'] = X[:, :, :, band].mean(axis=(1, 2))\n        df1[f'band{band+1}_std']  = X[:, :, :, band].std(axis=(1, 2))\n        df1[f'band{band+1}_min']  = X[:, :, :, band].min(axis=(1, 2))\n        df1[f'band{band+1}_max']  = X[:, :, :, band].max(axis=(1, 2))\n\n    # Derived features (NDVI, NDWI, radar)\n    for name, idx in zip(['ndvi', 'ndwi'], [12, 13]):\n        df[f'{name}_mean'] = X[:, :, :, idx].mean(axis=(1, 2))\n        df[f'{name}_std']  = X[:, :, :, idx].std(axis=(1, 2))\n        df[f'{name}_min']  = X[:, :, :, idx].min(axis=(1, 2))\n        df[f'{name}_max']  = X[:, :, :, idx].max(axis=(1, 2))\n\n    return df1\n\n\n# Apply to training dataframe\ntraining_df = means(train_df, X_features)\ntesting_df = means(test_df, X_test_features)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nlgb_model = XGBClassifier(random_state=42)\n\nX_df = training_df.drop(columns=['ID', 'label'])\ny_df = training_df['label']\n\nselector = SelectFromModel(lgb_model, threshold='median', prefit=False)\nselector.fit(X_df, y_df)\n# Get selected features\nselected_features = X_df.columns[selector.get_support()]\nX_selected = X_df[selected_features]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X_df, y_df, stratify=y_df, random_state=42)\nxgb =XGBClassifier(random_state=42)\nxgb.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T17:43:28.504607Z","iopub.execute_input":"2025-07-29T17:43:28.504973Z","iopub.status.idle":"2025-07-29T17:43:29.138794Z","shell.execute_reply.started":"2025-07-29T17:43:28.504946Z","shell.execute_reply":"2025-07-29T17:43:29.137863Z"}},"outputs":[{"execution_count":105,"output_type":"execute_result","data":{"text/plain":"XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=42, ...)","text/html":"<style>#sk-container-id-16 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-16 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-16 pre {\n  padding: 0;\n}\n\n#sk-container-id-16 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-16 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-16 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-16 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-16 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-16 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-16 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-16 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-16 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-16 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-16 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-16 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-16 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-16 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-16 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-16 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-16 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-16 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-16 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-16 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-16 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-16 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-16 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-16 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-16 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-16 div.sk-label label.sk-toggleable__label,\n#sk-container-id-16 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-16 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-16 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-16 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-16 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-16 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-16 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-16 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-16 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-16 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-16 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-16 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-16 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-16\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=42, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" checked><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;XGBClassifier<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=42, ...)</pre></div> </div></div></div></div>"},"metadata":{}}],"execution_count":105},{"cell_type":"code","source":"def cross_validate_model(model, X, y, n_splits=5, random_state=42):\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n    \n    oof_preds = np.zeros_like(y, dtype=float)\n    f1_scores, acc_scores = [], []\n    best_thresholds = []\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n        print(f\"\\n🔹 Fold {fold+1}/{n_splits}\")\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        # ✅ Fit model\n        model.fit(X_train, y_train)\n\n        # ✅ Predict probabilities if available\n        if hasattr(model, \"predict_proba\"):\n            y_prob = model.predict_proba(X_val)[:, 1]\n        else:\n            y_prob = model.predict(X_val)\n\n        # ✅ Threshold tuning\n        thresholds = np.arange(0.1, 0.9, 0.05)\n        best_f1, best_thresh = 0, 0.5\n        for thresh in thresholds:\n            y_pred_bin = (y_prob > thresh).astype(int)\n            f1 = f1_score(y_val, y_pred_bin)\n            if f1 > best_f1:\n                best_f1 = f1\n                best_thresh = thresh\n\n        print(f\"Best threshold for Fold {fold+1}: {best_thresh:.2f}\")\n\n        # ✅ Apply best threshold\n        y_pred = (y_prob > best_thresh).astype(int)\n        oof_preds[val_idx] = y_pred\n        best_thresholds.append(best_thresh)\n\n        # ✅ Metrics\n        acc = accuracy_score(y_val, y_pred)\n        f1_scores.append(best_f1)\n        acc_scores.append(acc)\n\n        print(f\"Fold {fold+1} F1: {best_f1:.4f} | Accuracy: {acc:.4f}\")\n\n    print(\"=\"*50)\n    print(f\"Mean F1 Score: {np.mean(f1_scores):.4f}\")\n    print(f\"Mean Accuracy: {np.mean(acc_scores):.4f}\")\n    print(f\"Average Best Threshold: {np.mean(best_thresholds):.2f}\")\n\ncross_validate_model(xgb, X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T17:49:03.872096Z","iopub.execute_input":"2025-07-29T17:49:03.872568Z","iopub.status.idle":"2025-07-29T17:49:06.813514Z","shell.execute_reply.started":"2025-07-29T17:49:03.872537Z","shell.execute_reply":"2025-07-29T17:49:06.812342Z"}},"outputs":[{"name":"stdout","text":"\n🔹 Fold 1/5\nBest threshold for Fold 1: 0.30\nFold 1 F1: 0.8351 | Accuracy: 0.9422\n\n🔹 Fold 2/5\nBest threshold for Fold 2: 0.35\nFold 2 F1: 0.8462 | Accuracy: 0.9478\n\n🔹 Fold 3/5\nBest threshold for Fold 3: 0.15\nFold 3 F1: 0.8142 | Accuracy: 0.9319\n\n🔹 Fold 4/5\nBest threshold for Fold 4: 0.35\nFold 4 F1: 0.8347 | Accuracy: 0.9431\n\n🔹 Fold 5/5\nBest threshold for Fold 5: 0.25\nFold 5 F1: 0.8041 | Accuracy: 0.9282\n==================================================\nMean F1 Score: 0.8269\nMean Accuracy: 0.9386\nAverage Best Threshold: 0.28\n","output_type":"stream"}],"execution_count":116},{"cell_type":"code","source":"def evaluate_model(model, X_test, y_test):\n\n    # Predict probabilities\n    y_pred = model.predict(X_test)\n\n    # Compute F1 score\n    f1 = f1_score(y_test, y_pred)\n    print(f\"F1 Score: {f1:.4f}\\n\")\n\n    # Confusion Matrix\n    cm = confusion_matrix(y_test, y_pred)\n    plt.figure(figsize=(6,5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n\n    # Detailed report\n    print(classification_report(y_test, y_pred))\n\n\nevaluate_model(xgb, X_val, y_val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T17:44:05.162341Z","iopub.execute_input":"2025-07-29T17:44:05.163249Z","iopub.status.idle":"2025-07-29T17:44:05.384681Z","shell.execute_reply.started":"2025-07-29T17:44:05.163216Z","shell.execute_reply":"2025-07-29T17:44:05.383565Z"}},"outputs":[{"name":"stdout","text":"F1 Score: 0.7973\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x500 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.95      0.98      0.96      1473\n           1       0.87      0.74      0.80       314\n\n    accuracy                           0.93      1787\n   macro avg       0.91      0.86      0.88      1787\nweighted avg       0.93      0.93      0.93      1787\n\n","output_type":"stream"}],"execution_count":108},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1, 10),\n        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True),\n        \"random_state\": 42,\n\n    }\n\n    f1_scores = []\n\n    model = XGBClassifier(**params)\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        verbose=False\n        )\n\n    y_pred = model.predict(X_val)\n    f1_scores.append(f1_score(y_val, y_pred))\n\n    return np.mean(f1_scores)\n\noptuna.logging.set_verbosity(optuna.logging.CRITICAL)\n\nxgb_study = optuna.create_study(direction=\"maximize\", sampler=RandomSampler())\nxgb_study.optimize(objective, n_trials=30)\n\nprint(\"Best F1:\", xgb_study.best_value)\nprint(\"Best Params:\", xgb_study.best_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T17:49:41.060945Z","iopub.execute_input":"2025-07-29T17:49:41.062015Z","iopub.status.idle":"2025-07-29T17:50:01.826693Z","shell.execute_reply.started":"2025-07-29T17:49:41.061917Z","shell.execute_reply":"2025-07-29T17:50:01.825694Z"}},"outputs":[{"name":"stdout","text":"Best F1: 0.8135593220338984\nBest Params: {'max_depth': 7, 'learning_rate': 0.12480477684810869, 'subsample': 0.9175940342290952, 'colsample_bytree': 0.7055597038362841, 'min_child_weight': 6.996259522364088, 'gamma': 1.5791372365762641, 'reg_lambda': 0.0014633739156366243, 'reg_alpha': 0.08155164899748174}\n","output_type":"stream"}],"execution_count":117},{"cell_type":"code","source":"best_xgb = XGBClassifier(**xgb_study.best_params)\ncross_validate_model(best_xgb, X_df, y_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T17:53:11.620549Z","iopub.execute_input":"2025-07-29T17:53:11.621564Z","iopub.status.idle":"2025-07-29T17:53:13.965991Z","shell.execute_reply.started":"2025-07-29T17:53:11.621526Z","shell.execute_reply":"2025-07-29T17:53:13.964854Z"}},"outputs":[{"name":"stdout","text":"\n🔹 Fold 1/5\nBest threshold for Fold 1: 0.50\nFold 1 F1: 0.8120 | Accuracy: 0.9385\n\n🔹 Fold 2/5\nBest threshold for Fold 2: 0.30\nFold 2 F1: 0.8228 | Accuracy: 0.9371\n\n🔹 Fold 3/5\nBest threshold for Fold 3: 0.40\nFold 3 F1: 0.8494 | Accuracy: 0.9496\n\n🔹 Fold 4/5\nBest threshold for Fold 4: 0.45\nFold 4 F1: 0.8245 | Accuracy: 0.9377\n\n🔹 Fold 5/5\nBest threshold for Fold 5: 0.45\nFold 5 F1: 0.8364 | Accuracy: 0.9433\n==================================================\nMean F1 Score: 0.8290\nMean Accuracy: 0.9412\nAverage Best Threshold: 0.42\n","output_type":"stream"}],"execution_count":118},{"cell_type":"code","source":"def threshold_tuning(model, X_test, y_test):\n    scores = []\n    thresholds = np.linspace(0.1, 1, 10)\n    for thresh in thresholds:\n        y_pred = model.predict_proba(X_test)[:, 1]\n        y_tuned = (y_pred > thresh).astype(int)\n        scores.append(f1_score(y_test, y_tuned))\n\n    fig = px.line(x=thresholds, y=scores, title='Threshold Tuning', template='plotly_white')\n    fig.update_traces(line=dict(color='red'))\n    fig.update_layout(xaxis_title='Thresholds',\n                      yaxis_title='F1 Scores',\n                      height=500,\n                      width=800)\n    \n    fig.show()\n\nthreshold_tuning(best_xgb, X_val, y_val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T17:45:42.046942Z","iopub.execute_input":"2025-07-29T17:45:42.047734Z","iopub.status.idle":"2025-07-29T17:45:42.233831Z","shell.execute_reply.started":"2025-07-29T17:45:42.047700Z","shell.execute_reply":"2025-07-29T17:45:42.232748Z"}},"outputs":[{"output_type":"display_data","data":{"text/html":"<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"810794bd-c6c9-4fb1-9755-e8b91a2381e2\" class=\"plotly-graph-div\" style=\"height:500px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"810794bd-c6c9-4fb1-9755-e8b91a2381e2\")) {                    Plotly.newPlot(                        \"810794bd-c6c9-4fb1-9755-e8b91a2381e2\",                        [{\"hovertemplate\":\"x=%{x}\\u003cbr\\u003ey=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"red\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[0.1,0.2,0.30000000000000004,0.4,0.5,0.6,0.7000000000000001,0.8,0.9,1.0],\"xaxis\":\"x\",\"y\":[0.8924889543446245,0.9409937888198758,0.9570747217806042,0.9612903225806452,0.954248366013072,0.9471947194719472,0.936026936026936,0.8931698774080561,0.7907869481765835,0.0],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Thresholds\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"F1 Scores\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Threshold Tuning\"},\"height\":500,\"width\":800},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('810794bd-c6c9-4fb1-9755-e8b91a2381e2');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                            </script>        </div>\n</body>\n</html>"},"metadata":{}}],"execution_count":112},{"cell_type":"code","source":"import shap\n\nexplainer = shap.TreeExplainer(best_xgb)\nshap_values = explainer.shap_values(X_train)\n\nshap.summary_plot(shap_values, X_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LGB","metadata":{}},{"cell_type":"code","source":"# 1️⃣ Train LightGBM\nlgb_model = LGBMClassifier(n_estimators=500, max_depth=-1, learning_rate=0.05)\nlgb_model.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T17:39:54.020931Z","iopub.execute_input":"2025-07-29T17:39:54.021674Z","iopub.status.idle":"2025-07-29T17:39:55.277264Z","shell.execute_reply.started":"2025-07-29T17:39:54.021637Z","shell.execute_reply":"2025-07-29T17:39:55.276199Z"}},"outputs":[{"execution_count":95,"output_type":"execute_result","data":{"text/plain":"LGBMClassifier(learning_rate=0.05, n_estimators=500)","text/html":"<style>#sk-container-id-14 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-14 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-14 pre {\n  padding: 0;\n}\n\n#sk-container-id-14 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-14 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-14 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-14 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-14 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-14 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-14 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-14 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-14 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-14 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-14 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-14 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-14 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-14 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-14 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-14 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-14 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-14 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-14 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-14 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-14 div.sk-label label.sk-toggleable__label,\n#sk-container-id-14 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-14 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-14 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-14 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-14 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-14 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-14 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-14 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-14 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-14 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-14 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-14 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(learning_rate=0.05, n_estimators=500)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LGBMClassifier<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMClassifier(learning_rate=0.05, n_estimators=500)</pre></div> </div></div></div></div>"},"metadata":{}}],"execution_count":95},{"cell_type":"code","source":"cross_validate_model(lgb_model, X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T17:53:44.934917Z","iopub.execute_input":"2025-07-29T17:53:44.935375Z","iopub.status.idle":"2025-07-29T17:53:55.766123Z","shell.execute_reply.started":"2025-07-29T17:53:44.935346Z","shell.execute_reply":"2025-07-29T17:53:55.764571Z"}},"outputs":[{"name":"stdout","text":"\n🔹 Fold 1/5\nBest threshold for Fold 1: 0.15\nFold 1 F1: 0.8482 | Accuracy: 0.9459\n\n🔹 Fold 2/5\nBest threshold for Fold 2: 0.15\nFold 2 F1: 0.8571 | Accuracy: 0.9487\n\n🔹 Fold 3/5\nBest threshold for Fold 3: 0.40\nFold 3 F1: 0.8255 | Accuracy: 0.9412\n\n🔹 Fold 4/5\nBest threshold for Fold 4: 0.20\nFold 4 F1: 0.8368 | Accuracy: 0.9422\n\n🔹 Fold 5/5\nBest threshold for Fold 5: 0.15\nFold 5 F1: 0.8154 | Accuracy: 0.9328\n==================================================\nMean F1 Score: 0.8366\nMean Accuracy: 0.9422\nAverage Best Threshold: 0.21\n","output_type":"stream"}],"execution_count":119},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 1000),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 100),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 50),\n        \"verbose\": -1\n    }\n\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    f1_scores = []\n\n\n    model = LGBMClassifier(**params, random_state=42)\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        eval_metric='binary_logloss',\n        callbacks=[]\n        )\n\n    y_pred = model.predict(X_val)\n    f1_scores.append(f1_score(y_val, y_pred))\n\n    return np.mean(f1_scores)\n\n    return np.mean(f1_scores)\n\noptuna.logging.set_verbosity(optuna.logging.CRITICAL)\n\nlgbm_study = optuna.create_study(direction=\"maximize\")\nlgbm_study.optimize(objective, n_trials=30)\n\nprint(\"Best F1:\", lgbm_study.best_value)\nprint(\"Best Params:\", lgbm_study.best_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T17:54:21.988388Z","iopub.execute_input":"2025-07-29T17:54:21.989067Z","iopub.status.idle":"2025-07-29T17:55:18.971140Z","shell.execute_reply.started":"2025-07-29T17:54:21.989022Z","shell.execute_reply":"2025-07-29T17:55:18.970024Z"}},"outputs":[{"name":"stdout","text":"Best F1: 0.8253424657534246\nBest Params: {'n_estimators': 239, 'learning_rate': 0.27674553468371743, 'max_depth': 10, 'num_leaves': 67, 'subsample': 0.8912130192605716, 'colsample_bytree': 0.6792962894751995, 'min_child_samples': 35}\n","output_type":"stream"}],"execution_count":120},{"cell_type":"code","source":"best_lgbm = LGBMClassifier(**lgbm_study.best_params)\ncross_validate_model(best_lgbm, X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T17:58:22.940708Z","iopub.execute_input":"2025-07-29T17:58:22.941138Z","iopub.status.idle":"2025-07-29T17:58:26.675549Z","shell.execute_reply.started":"2025-07-29T17:58:22.941113Z","shell.execute_reply":"2025-07-29T17:58:26.674563Z"}},"outputs":[{"name":"stdout","text":"\n🔹 Fold 1/5\nBest threshold for Fold 1: 0.10\nFold 1 F1: 0.8320 | Accuracy: 0.9412\n\n🔹 Fold 2/5\nBest threshold for Fold 2: 0.10\nFold 2 F1: 0.8503 | Accuracy: 0.9478\n\n🔹 Fold 3/5\nBest threshold for Fold 3: 0.35\nFold 3 F1: 0.8245 | Accuracy: 0.9412\n\n🔹 Fold 4/5\nBest threshold for Fold 4: 0.40\nFold 4 F1: 0.8315 | Accuracy: 0.9440\n\n🔹 Fold 5/5\nBest threshold for Fold 5: 0.30\nFold 5 F1: 0.8194 | Accuracy: 0.9375\n==================================================\nMean F1 Score: 0.8315\nMean Accuracy: 0.9424\nAverage Best Threshold: 0.25\n","output_type":"stream"}],"execution_count":121},{"cell_type":"markdown","source":"## Block 9: Make Predictions on the Test Set and Prepare Submission","metadata":{"id":"rxJ3bFl30Jbe"}},{"cell_type":"markdown","source":"## Ensemble Models","metadata":{}},{"cell_type":"code","source":"def blend_models_f1(model1, model2, X, y, n_splits=5):\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(y))  # out-of-fold predictions\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n        print(f\"\\n🔹 Fold {fold}/{n_splits}\")\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        \n        # Clone models for each fold to avoid contamination\n        m1 = model1.__class__(**model1.get_params())\n        m2 = model2.__class__(**model2.get_params())\n        \n        # Train both models\n        m1.fit(X_train, y_train)\n        m2.fit(X_train, y_train)\n        \n        # Get predictions\n        p1 = m1.predict_proba(X_val)[:, 1]\n        p2 = m2.predict_proba(X_val)[:, 1]\n        \n        # Average predictions\n        avg_pred = (p1 + p2) / 2\n        \n        # Threshold at 0.5 for binary classification\n        oof_preds[val_idx] = (avg_pred > 0.4).astype(int)\n        \n        fold_f1 = f1_score(y_val, oof_preds[val_idx])\n        print(f\"Fold {fold} F1 Score: {fold_f1:.4f}\")\n    \n    overall_f1 = f1_score(y, oof_preds)\n    print(f\"\\n✅ Overall F1 Score: {overall_f1:.4f}\")\n\n\nblend_models_f1(best_xgb, best_lgbm, X_df, y_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Single Model","metadata":{}},{"cell_type":"code","source":"def stratifiedKFold(model):\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    f1_scores = []\n    test_preds = []\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(X_df, y_df)):\n        print(f\"Fold {fold+1}\")\n    \n        # Use the same DataFrames as in split\n        X_tr, X_val = X_df.iloc[train_idx], X_df.iloc[val_idx]\n        y_tr, y_val = y_df.iloc[train_idx], y_df.iloc[val_idx]\n    \n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_val, y_val)],\n        )\n    \n        # Validation predictions and F1\n        y_pred_val = model.predict(X_val)\n        f1 = f1_score(y_val, y_pred_val)\n        f1_scores.append(f1)\n        print(f\"Fold {fold+1} F1: {f1:.4f}\")\n    \n        # Test predictions (probabilities)\n        y_pred_test = model.predict_proba(X_preds[selected_features])[:, 1]\n        test_preds.append(y_pred_test)\n    \n    # ✅ Average test predictions from 5 folds\n    final_test_preds = np.mean(test_preds, axis=0)\n    \n    # ✅ Convert to class labels with custom threshold\n    final_labels = (final_test_preds >= 0.4).astype(int)\n    \n    print(\"CV Mean F1:\", np.mean(f1_scores))\n    return final_labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#y_pred = stratifiedKFold(best_xgb)\ny_pred = stratifiedKFold(lgb_model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def blend_models_kfold(model1, model2, X_train, y_train, X_test, n_splits=5):\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n    test_preds_folds = []\n    oof_preds = np.zeros(len(y_train))\n\n    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n        print(f\"\\n🔹 Fold {fold}/{n_splits}\")\n\n        X_tr, X_val = X_train.iloc[tr_idx], X_train.iloc[val_idx]\n        y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n\n        # Clone models per fold\n        m1 = model1.__class__(**model1.get_params())\n        m2 = model2.__class__(**model2.get_params())\n\n        # Train models\n        m1.fit(X_tr, y_tr)\n        m2.fit(X_tr, y_tr)\n\n        # Predict probabilities\n        p1_val = m1.predict_proba(X_val)[:, 1]\n        p2_val = m2.predict_proba(X_val)[:, 1]\n        val_avg = (p1_val + p2_val) / 2\n        oof_preds[val_idx] = val_avg\n\n        # Predict test set probabilities\n        p1_test = m1.predict_proba(X_test)[:, 1]\n        p2_test = m2.predict_proba(X_test)[:, 1]\n        test_avg = (p1_test + p2_test) / 2\n        test_preds_folds.append(test_avg)\n\n    # Average predictions across folds\n    test_preds_final = np.mean(test_preds_folds, axis=0)\n\n    final_preds_binary = (test_preds_final > 0.4).astype(int)\n\n    return final_preds_binary\n\ny_pred = blend_models_kfold(best_lgbm, best_xgb, X_df, y_df, X_preds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_ids = test_df['ID'].values\n# Predict probabilities and classify as 0 or 1\n\n# Count the number of predictions for each class\nunique, counts = np.unique(y_pred, return_counts=True)\nprediction_counts = dict(zip(unique, counts))\nprint(\"Prediction counts:\", prediction_counts)\n\n# Prepare submission file\nsubmission_df = pd.DataFrame({\n    'ID': test_ids,\n    'label': y_pred.flatten()  # Flatten to match submission format\n})\nsubmission_df.to_csv('/kaggle/working/Submission_File.csv', index=False)\nprint(\"Sample submission file created as 'Submission_File.csv'.\")","metadata":{"id":"H7GNYIMl0Jbe","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import plot_importance\nplot_importance(best_xgb, importance_type='gain', max_num_features=20)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Transfer Learning","metadata":{}},{"cell_type":"markdown","source":"### Explanation\n1. **Count Predictions**:\n   - After making predictions on `X_test`, we use `np.unique` with `return_counts=True` to count the occurrences of `0`s and `1`s in `y_test_pred`.\n   - We print the counts, which shows the distribution of predicted labels.\n\n2. **Interpretation**:\n   - The counts provide insight into whether the model is predicting a balanced number of `0`s and `1`s or if it's skewed towards one class.\n   - Please consider that the test set is imbalanced towards the non-landslide class.\n   - This check is particularly useful for binary classification problems where class imbalance could impact the model’s evaluation.\n\n3. **Prepare Submission File**:\n   - The `Submission_File.csv` file is created in the same way, ready for submission.","metadata":{"id":"DeyImf6L0Jbf"}}]}